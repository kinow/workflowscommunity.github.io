[
  
    {
      "title"       : "Apollo Runtime System",
      "url"         : "/systems/EE-Demo/",
      "description" : "A runtime system for the orchestration of workflows across the cloud-edge-Iot continuum.",
      "tags"        : "",
      "category"    : "system"
    },
  
    {
      "title"       : "AiiDA",
      "url"         : "/systems/aiida-core/",
      "description" : "A workflow manager for computational science with a strong focus on provenance, performance and extensibility.",
      "tags"        : "aiida, computational-science, data-provenance, database, provenance, scheduler, ssh, workflow, workflow-engine,  workflows",
      "category"    : "system"
    },
  
    {
      "title"       : "Makeflow",
      "url"         : "/systems/cctools/",
      "description" : "Makeflow is a workflow system for parallel and distributed computing that uses a language very similar to Make.",
      "tags"        : "",
      "category"    : "system"
    },
  
    {
      "title"       : "Common Workflow Language",
      "url"         : "/systems/common-workflow-language/",
      "description" : "Interoperable workflow execution of containerized command line tools",
      "tags"        : "common-workflow-language, commonwl, containers, cwl, science, sciworkflows, workflow,  workflows",
      "category"    : "system"
    },
  
    {
      "title"       : "PyCOMPSs/COMPSs",
      "url"         : "/systems/compss/",
      "description" : "Easy task-based parallelization and efficient execution in distributed infrastructures.",
      "tags"        : "c, distributed-computing, docker, hpc, java, pipeline-framework, python, singularity, slurm, workflow-management-system,  workflows",
      "category"    : "system"
    },
  
    {
      "title"       : "Covalent",
      "url"         : "/systems/covalent/",
      "description" : "Distributed workflows for quantum and HPC",
      "tags"        : "covalent, data-pipeline, data-science, etl, hacktoberfest, hpc, hpc-applications, machine-learning, machinelearning, machinelearning-python, orchestration, parallelization, pipelines, python, quantum, quantum-computing, quantum-machine-learning, workflow, workflow-automation,  workflow-management",
      "category"    : "system"
    },
  
    {
      "title"       : "cromwell",
      "url"         : "/systems/cromwell/",
      "description" : "Scientific workflow engine designed for simplicity &amp; scalability. Trivially transition between one off use cases to massive scale production environments",
      "tags"        : "application, bioinformatics, cloud, common-workflow-language, containers, cwl, docker, executor, ga4gh, hpc, scala, wdl, workflow, workflow-description-language,  workflow-execution",
      "category"    : "system"
    },
  
    {
      "title"       : "dask",
      "url"         : "/systems/dask/",
      "description" : "Parallel computing with task scheduling",
      "tags"        : "dask, numpy, pandas, pydata, python, scikit-learn,  scipy",
      "category"    : "system"
    },
  
    {
      "title"       : "fireworks",
      "url"         : "/systems/fireworks/",
      "description" : "The Fireworks Workflow Management Repo.",
      "tags"        : "",
      "category"    : "system"
    },
  
    {
      "title"       : "galaxy",
      "url"         : "/systems/galaxy/",
      "description" : "Data intensive science for everyone.",
      "tags"        : "bioinformatics, dna, docker, genomics, hacktoberfest, ngs, pipeline, science, sequencing, usegalaxy, workflow,  workflow-engine",
      "category"    : "system"
    },
  
    {
      "title"       : "Geoweaver",
      "url"         : "/systems/geoweaver/",
      "description" : "a lightweight workflow software to easily orchestrate pipelines from Python and shell scripts and preserve history of every execution",
      "tags"        : "ai, docker, earth-science, esip-lab, google-earth-engine, jupyter-hub, jupyter-lab, jupyter-notebook, kubernetes, pangeo, pipeline, pipeline-framework, proxy, scientific-computing, workflow, workflow-engine, workflow-management,  workflow-tool",
      "category"    : "system"
    },
  
    {
      "title"       : "libEnsemble",
      "url"         : "/systems/libensemble/",
      "description" : "Tool for running dynamic ensembles.",
      "tags"        : "",
      "category"    : "system"
    },
  
    {
      "title"       : "Maestro Workflow Conductor",
      "url"         : "/systems/maestrowf/",
      "description" : "Orchestrate your HPC workflows with ease",
      "tags"        : "hpc, python, radiuss, science-research, simulation-study, workflow, workflow-processes,  workflows",
      "category"    : "system"
    },
  
    {
      "title"       : "Merlin",
      "url"         : "/systems/merlin/",
      "description" : "Enabling Machine Learning HPC Workflows",
      "tags"        : "big-data, celery-workers, hpc, machine-learning, radiuss, redis-server, simulation, workflow,  workflows",
      "category"    : "system"
    },
  
    {
      "title"       : "mlflow",
      "url"         : "/systems/mlflow/",
      "description" : "Open source platform for the machine learning lifecycle",
      "tags"        : "ai, apache-spark, machine-learning, ml, mlflow,  model-management",
      "category"    : "system"
    },
  
    {
      "title"       : "nextflow",
      "url"         : "/systems/nextflow/",
      "description" : "A DSL for data-driven computational pipelines",
      "tags"        : "aws, bioinformatics, cloud, dataflow, docker, groovy, hello, hpc, nextflow, pipeline, pipeline-framework, reproducible-research, reproducible-science, sge, singularity, singularity-containers, slurm,  workflow-engine",
      "category"    : "system"
    },
  
    {
      "title"       : "nnodes",
      "url"         : "/systems/nnodes/",
      "description" : "None",
      "tags"        : "",
      "category"    : "system"
    },
  
    {
      "title"       : "Parsl",
      "url"         : "/systems/parsl/",
      "description" : "Productive parallel programming in Python",
      "tags"        : "hacktoberfest",
      "category"    : "system"
    },
  
    {
      "title"       : "pegasus",
      "url"         : "/systems/pegasus/",
      "description" : "Pegasus Workflow Management System - Automate, recover, and debug scientific computations.",
      "tags"        : "bioinformatics, distributed-systems, hpc, workflow,  workflow-management-system",
      "category"    : "system"
    },
  
    {
      "title"       : "radical.entk",
      "url"         : "/systems/radical.entk/",
      "description" : "The RADICAL Ensemble Toolkit",
      "tags"        : "",
      "category"    : "system"
    },
  
    {
      "title"       : "SciPipe",
      "url"         : "/systems/scipipe/",
      "description" : "Robust, flexible and resource-efficient pipelines using Go and the commandline.",
      "tags"        : "bioinformatics, bioinformatics-pipeline, cheminformatics, dataflow, fbp, go, pipeline, scientific-workflows, scipipe, workflow,  workflow-engine",
      "category"    : "system"
    },
  
    {
      "title"       : "Snakemake",
      "url"         : "/systems/snakemake/",
      "description" : "This is the development home of the workflow management system Snakemake. For general information, see",
      "tags"        : "reproducibility, snakemake,  workflow-management",
      "category"    : "system"
    },
  
    {
      "title"       : "StreamFlow",
      "url"         : "/systems/streamflow/",
      "description" : "Towards Cloud-HPC Continuum",
      "tags"        : "workflows",
      "category"    : "system"
    },
  
    {
      "title"       : "swift-t",
      "url"         : "/systems/swift-t/",
      "description" : "Swift/T: High Performance Parallel Scripting Language",
      "tags"        : "",
      "category"    : "system"
    },
  
  
    {
      "title"       : "Postdoctoral Appointee - High-Performance Computing System and AI for X-Ray Science",
      "url"         : "/jobs/anl_postdoctoral_hpc/",
      "description" : "OverviewWe are currently seeking a Postdoctoral Researcher to work at the intersection of machine learning and high-performance computing(HPC) systems, and develop artificial intelligence (AI) and machine learning (ML) solutions for DOE scientific user facilities. In particular, the project aims to develop AI/ML models and infrastructure to extract  features in X-ray detector data at the Advanced Photon Source (APS) at Argonne and Linac Coherent Light Source (LCLS) at SLAC. Because such models can run at high speeds, thanks to advances in AI  accelerators, it becomes feasible to extract salient information from in-flight data, in real time, and thus both enabling fast feedback and reducing downstream computational burden.In this role you will conduct cutting-edge research in data science and deep learning applied to scientific problems in X-ray science, and play a key roles in developing physics-based AI/ML models, developing workflow and implement rapid ML model training on data center AI systems (e.g., ALCF AI Testbed and Argonne’s Aurora exascale supercomputer), end-to-end model training workflows and explore AI accelerators for simulation applications.Position Requirements  Recent PhD (typically completed within 0-3 years) in a computer science, physical sciences or engineering or related field.  Comprehensive experience programming in one or more programming languages, such as Python.  Experience with machine learning methods and deep learning frameworks, including tensorflow, pytorch.  Software development practices and techniques for computational and data-intensive science problems.  Experience in interdisciplinary research involving computer and material scientists.  Experience with applied machine learning (e.g., successful projects that used ML to resolve scientific problems).  Experience with high-performance computing and/or scientific workflow.  Exceptional communication skills, ability to communicate effectively with internal and external collaborators and ability to work in team environment.  Ability to model Argonne’s Core Values: Impact, Safety, Respect, Integrity, and Teamwork.Job FamilyPostdoctoral FamilyJob ProfilePostdoctoral AppointeeWorker TypeLong-Term (Fixed Term)Time TypeFull timeAs an equal employment opportunity and affirmative action employer, and in accordance with our core values of impact, safety, respect, integrity and teamwork, Argonne National Laboratory is committed to a diverse and inclusive workplace that fosters collaborative scientific discovery and innovation. In support of this commitment, Argonne encourages minorities, women, veterans and individuals with disabilities to apply for employment. Argonne considers all qualified applicants for employment without regard to age, ancestry, citizenship status, color, disability, gender, gender identity, gender expression, genetic information, marital status, national origin, pregnancy, race, religion, sexual orientation, veteran status or any other characteristic protected by law.Argonne employees, and certain guest researchers and contractors, are subject to particular restrictions related to participation in Foreign Government Sponsored or Affiliated Activities, as defined and detailed in United States Department of Energy Order 486.1A. You will be asked to disclose any such participation in the application phase for review by Argonne’s Legal Department.All Argonne offers of employment are contingent upon a background check that includes an assessment of criminal conviction history conducted on an individualized and case-by-case basis.  Please be advised that Argonne positions require upon hire (or may require in the future) for the individual be to obtain a government access authorization that involves additional background check requirements.  Failure to obtain or maintain such government access authorization could result in the withdrawal of a job offer or future termination of employment.Please note that all Argonne employees are required to be vaccinated against COVID-19. All successful applicants will be required to provide their COVID-19 vaccination verification as a condition of employment, subject to limited legally recognized exemptions to COVID-19 vaccination.",
      "category"    : "job"
    },
  
    {
      "title"       : "Nextflow Developer",
      "url"         : "/jobs/ardigen_nextflow_developer/",
      "description" : "About the jobArdigen is harnessing advanced Artificial Intelligence methods for novel precision medicine. The company accelerates therapy development by designing immunity, decoding microbiome, analysing biomedical images and providing digital drug discovery services. Ardigen’s team is rooted in biology and holds deep expertise in bioinformatics, machine learning, and software engineering. The company’s in-house datasets together with advanced AI platforms empower the development of effective precision therapies.Let’s grow together &amp;amp; code against cancer!Job description:  Development of scripts and pipelines that transform and aggregate data to create new scientific workflows  Maintenance of existing pipelines written in Nextflow  Development of new pipelines in Nextflow  Work in multidisciplinary teams: software developers, bioinformaticians and data scientistsRequirements  Knowledge of pipeline building tools / workflow composition tools e.g. Jenkins, Groovy, Nextflow, Snakemake, etc.  Knowledge of relational databases (eg. MySQL, PostgreSQL)  Documented work experience at least 2-3 years  Familiarity with distributed version control systems (Git)  Experience with automated unit and integration testing  Proficient in English with good written and verbal communication skillsYou get bonus points for  Knowledge of Python  Experience working with AWS cloud services or other Clouds (S3, EC2, Batch)  Familiarity with DockerWe offer  An opportunity to shape the breakthrough solutions in the fight against cancer  Permanent position that can be adjusted to your preferences in terms of employment form (“UoP”/B2B)  Fully remote or hybrid working model  Flexible working hours  Mental health support  Multisport Card, Private Healthcare (LuxMed), Life Insurance  Yearly bonus up to 20% of your annual income  Clearly defined career paths, funding for professional development, trainings and conference attendance  Mentoring program  Internal library  Knowledge sharing sessions, company events, meetups, webinars, activity groups (professional, sports, hobbies)  By cooperating with a non-profit Foundation #CodeAgainstCancer we have a real impact in helping children with cancer  Work with world-class companies and institutions on the cutting-edge of Life Science industry  Experts community of tech specialists, scientists and people with passion for their craft",
      "category"    : "job"
    },
  
    {
      "title"       : "Computer Scientist",
      "url"         : "/jobs/bnl_computer_scientist/",
      "description" : "Brookhaven National Laboratory (www.bnl.gov) delivers discovery science and transformative technology to power and secure the nation’s future. Brookhaven Lab is a multidisciplinary laboratory with seven Nobel Prize-winning discoveries, 37 R&amp;amp;D 100 Awards, and more than 70 years of pioneering research. The Lab is primarily supported by the U.S. Department of Energy’s (DOE) Office of Science. Brookhaven Science Associates (BSA) operates and manages the Laboratory for DOE. BSA is a partnership between Battelle and The Research Foundation for the State University of New York on behalf of Stony Brook University. BSA salutes our veterans and active military members with careers that leverage the skills and unique experience they gained while serving our country. Our organization fully supports service members transitioning from active duty to civilian life and pledge’s our commitment to actively hire veterans of the U.S. Armed Forces. Military personnel who have been formally trained or have relevant experience obtained while in service may meet educational requirements and are encouraged to apply for job opportunities at BSA.Organizational OverviewC3D is the CSI gateway for expertise in high-performance workflows, distributed computing technologies, and FAIR (Findability, Accessibility, Interoperability, and Reusability) data. C3D integrates high-performance computing, machine learning, and streaming analytics, and translates them into reproducible science and scientific discovery. C3D conducts research, develops solutions, and provides expertise in workflows, scalable software, and analytics to address the challenges and requirements of science and engineering applications that demand large-scale, innovative solutions. C3D specializes in distributed computing research, software systems that support streaming and real-time analytics, and reproducibility. The team’s work includes performing advanced research into extreme-scale and extensible workflow systems, transparent and reproducible artificial intelligence systems in science, and real-time data analytics with a focus on explainability. Contact any C3D member for additional information and collaboration opportunities.Position DescriptionThis job provides an opportunity to contribute to the several impactful projects and software systems. These include DOE ECP funded CANDLE, ExaWorks and ExaLearn projects. The opportunity to research and develop the next generation of middleware systems to advance scientific discovery on the largest computing platforms.This position is available in the laboratory of Shantenu Jha, a recipient of the Gordon Bell Special Prize for COVID-19 research related work - https://www.hpcwire.com/2020/11/19/gordon-bell-special-prize-goes-to-massive-sars-cov-2-simulation/Essential Duties and Responsibilities  Help develop application and HPC middleware software frameworks.  Contribute to interdisciplinary and community software development team.  Promote the research results through scholarly publications and presentations at leading conferences  Lead or participate in the development of related research proposals.  Promote collaborative research with interdisciplinary research team  Develop frameworks and libraries for high-Performance computing and Machine Learning applications.  Integrate frameworks and libraries with scientific applications on DOE Leadership Facilities.Position RequirementsRequired Knowledge, Skills, and Abilities  Bachelor’s +5 years, Master’s +3, or Doctorate in Computer Science/Engineering, Statistics, Applied Math, Physics or related discipline  Experience in successful and modern software design and development and lifestyle methods.  Python and C programming experience  Experience with High-Performance Computing — principles, practice, programming and performance analysis  Strong Software Engineering Skills  Experience with Scientific Workflows and Resource Management  Experience working in multidisciplinary scientific collaboration  Track record of producing high quality software on schedulePreferred Knowledge, Skills, and Abilities  Doctorate +2 years post-PhD  Appreciation for a range of scientific domains: high-energy physics, bimolecular, climate sciences, etc.  Experience working in inter-disciplinary teams  A degree in CS/CE in high-performance computing  Experience in system software design and implementation of scalable systemsOther Information  Moderate domestic and foreign travel(Position level will be commensurate with qualifications and experience.)At Brookhaven National Laboratory we believe that a comprehensive employee benefits program is an important and meaningful part of the compensation employees receive. Our benefits program includes, but is not limited to:  Medical, Dental, and Vision Care Plans  Flexible Spending Accounts  Paid Time-off and Leave Programs (vacation, holidays, sick leave, paid parental leave)  Lab-funded Retirement Plan  401(k) Plan  Flexible Work Arrangements  Tuition Assistance, Training and Professional Development Programs  Employee Fitness/Wellness &amp;amp; Recreation:  Gym/Basketball Courts, Weight Room, Fitness Classes, Indoor Pool, Tennis Courts, Sports Clubs/Activities (Basketball, Ping Pong, Softball, Tennis)Brookhaven Science Associates requires proof of a COVID-19 vaccination for all employees. Proof of full vaccination as recognized by the CDC and/or WHO, inclusive of the two-week waiting period, is required at the start of your employment.Brookhaven National Laboratory (BNL) is an equal opportunity employer that values inclusion and diversity at our Lab. We are committed to ensuring that all qualified applicants receive consideration for employment and will not be discriminated against on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, status as a veteran, disability or any other federal, state or local protected class.BNL takes affirmative action in support of its policy and to advance in employment individuals who are minorities, women, protected veterans, and individuals with disabilities. We ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",
      "category"    : "job"
    },
  
    {
      "title"       : "Exascale Supercomputer Software Container Engineer (RE2)",
      "url"         : "/jobs/bsc_exascale_research_engineer/",
      "description" : "About BSCThe Barcelona Supercomputing Center - Centro Nacional de Supercomputación (BSC-CNS) is the leading supercomputing center in Spain. It houses MareNostrum, one of the most powerful supercomputers in Europe, and is a hosting member of the PRACE European distributed supercomputing infrastructure. The mission of BSC is to research, develop and manage information technologies in order to facilitate scientific progress. BSC combines HPC service provision and R&amp;amp;D into both computer and computational science (life, earth and engineering sciences) under one roof, and currently has over 770 staff from 55 countries.Look at the BSC experience:  BSC-CNS YouTube Channel  Let’s stay connected with BSC Folks!Context And MissionBSC is looking for a research engineer that contributes to the container support and to an ecosystem targeting RISC-V-based ISA for a European HPC accelerator. The position is funded by a project aiming to build the software infrastructure and toolchain for an FPGA-based emulator for an energy-efficient Exascale system.More specifically, the candidate will collaborate with the Workflows and Distributed Computing (WDC) group to validate container images running PyCOMPSs applications (compss.bsc.es). PyCOMPSs is a task-based programming model developed by the WDC group, that aims at easing the development of parallel applications for distributed computing. Between the applications to validate, the candidate will work with machine learning applications (dislib scripts, see dislib.bsc.es) parallelized with COMPSs. In addition, the candidate will support the whole project on its CD/CI infrastructure.Key Duties  Validation of COMPSs container images for RISC-V architecture  Validation of COMPSs applications on top of containers for RISC-V  Validate machine learning applications on the RISC-V architecture  Extend containers and the container ecosystem to RISC-V based hardware.  Build and maintain high-performance runtime frameworks, libraries, and servicesRequirementsEducation  Master in Computer Science, or related Engineering degree or equivalent level of professional experience.Essential Knowledge and Professional Experience  Experience with container technologies, such as Docker and orchestration platforms like Kubernetes.  Good knowledge of Linux OS (not necessarily all):          Linux filesystems      Image distribution      Content storage and management      Kernel and container security      Linux containerization      Additional Knowledge and Professional Experience  Knowledge on parallel and distributed computing  Experience with Continuous Integration/Continuous Deployment frameworks  Agile development and open source development, deployment, and support, including GitHub or equivalent  Open source software committer a plusCompetences  Effective communication, multitasking, and working well on collaborative designs.  Ability to think creatively.  Ability to take initiative, prioritize and work under set deadlines and pressure.  Fluency in English is essential, Spanish is welcome.Conditions  The position will be located at BSC within the Computer Sciences Department  We offer a full-time contract, a good working environment, a highly stimulating environment with state-of-the-art infrastructure, flexible working hours, extensive training plan, tickets restaurant, private health insurance, fully support to the relocation procedures  Duration: Temporary - 31/12/2023 renewable  Salary: we offer a competitive salary commensurate with the qualifications and experience of the candidate and according to the cost of living in Barcelona  Starting date: ASAPApplications procedure and processAll applications must be made through BSC website and contain:  A full CV in English including contact details  A Cover Letter with a statement of interest in English, including two contacts for further references - Applications without this document will not be consideredIn accordance with the OTM-R principles, a gender-balanced recruitment panel is formed for every vacancy at the beginning of the process. After reviewing the content of the applications, the panel will start the interviews, with at least one technical and one administrative interview. A profile questionnaire as well as a technical exercise may be required during the process.The panel will make a final decision and all candidates who had contacts with them will receive a feedback with details on the acceptance or rejection of their profile.At BSC we are seeking continuous improvement in our recruitment processes, for any suggestions or feedback/complaints about our Recruitment Processes, please contact recruitment@bsc.es.For more information follow this link.DeadlineThe vacancy will remain open until suitable candidate has been hired. Applications will be regularly reviewed and potential candidates will be contacted.OTM-R principles for selection processesBSC-CNS is committed to the principles of the Code of Conduct for the Recruitment of Researchers of the European Commission and the Open, Transparent and Merit-based Recruitment principles (OTM-R). This is applied for any potential candidate in all our processes, for example by creating gender-balanced recruitment planels and recognizing career breaks etc.BSC-CNS is an equal opportunity employer committed to diversity and inclusion. We are pleased to consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or any other basis protected by applicable state or local law.For more information follow this link.This position is reserved for candidates who meet the requirements and have the legal status of disabled persons with a degree of disability equal to or greater than 33%. In case there are no applicants with disabilities that meet the requirements, the rest of the candidates without declared disability will be evaluated.",
      "category"    : "job"
    },
  
    {
      "title"       : "Bioinformatics Software Engineer - Genomic Workflows",
      "url"         : "/jobs/genentech_bioinformatics_software_engineer/",
      "description" : "The PositionGenentech seeks a talented and highly motivated Bioinformatics Software Engineer with expert knowledge of modern genomic data processing workflows and their application to cutting-edge biological sciences. The team you are joining supports gRED scientists with highly scalable and reproducible workflows for the analysis of pre-clinical and clinical sequencing data from transcriptomic, genomic, and (epi)-genetic assays  at the bulk and single cell level.The primary focus of this position is to maintain and extend an orchestration system for high throughput genomic workflows to scale with the ever growing data volume and diversity as biology becomes a quantitative science. Your work will enable us to readily deploy our containerized workflows, with an emphasis on reproducibility and scalability, to public and private  cloud environments so we can compute where the data resides. You are well informed about evolving and established data standards, workflow frameworks and relevant open source tools. You can evaluate and incorporate these tools into our data platform and complement them with your own software as needed.A successful candidate will collaborate with interdisciplinary teams of Software Engineers, DevOps engineers and Computational Biology Scientists to deploy and execute genomic workflows on thousands of samples and hundreds of projects at any given time. You are adapting our platform to new compute environments and continually make it more performant and robust. You are guiding our workflow developers to optimize their  workflows for throughput, reliability  and cost.*This position may be eligible for relocation and visa sponsorshipSuccessful candidates will meet many of the following requirements  You have a PhD in Software Engineering, Computer Science, Bioinformatics, or similar and 2-3 years of relevant experience in a clinical, academic or commercial setting. Alternatively, a Master degree or equivalent and at least 5-8 years of relevant experience.  You have expert knowledge of a modern genomic framework system (ideally cromwell/WDL, alternatively CWL, nextflow, toil) and have not only written workflows but have looked behind the curtain and deployed or configured the framework in a public or private cloud environment or on a large HPC  You have expert knowledge of containerization in the context of genomic workflows, including automatic builds, CI/CD and  container registries (e.g. dockstore, quai.io)  You have successfully analyzed large cohorts using an automated workflow system and were involved in all aspects of the process from data acquisition, over workflow development to data processing and interpretation.  Experience in managing  data according to FAIR principles as well as tracking data lineage, ensuring data quality and improving data discovery is a plus as well as experience with data standards used by large genomics consortia.  You have successfully planned and implemented multiple software projects throughout the whole development cycle (planning, implementation, testing, release, maintenance) following modern software development practices.  You are able to break down large problems into smaller software components and can develop them independently or as part of a large team.  Experience in managing  data according to FAIR principles as well as tracking data lineage, ensuring data quality and improving data discovery is a plus.  You have strong experience in the use of a high-level, object oriented  programming language such as Python, Go or Java and a strong understanding of Linux/Unix.  You have knowledge of best practices for software engineering including but not limited to IaC, CI/CD, and software containerization.  You are comfortable working both independently and collaboratively, and with handling several concurrent, fast-paced projects.  You are able to execute technical projects at global scale, across multiple teams and time zones. Strategic, analytical mindset and ability to innovate using technology to advance business goals.  You have strong communication skills and can untangle and discuss complex technical tasks.  Prior experience  in a life science or drug development environment is beneficial.What to expect from us  A highly collaborative and dynamic research environment where we aim to advance the rate of scientific discovery using purposefully built solutions.  Access to large data sets, samples and compute resources.  Access to state-of-the-art technologies and pioneering research.  Participation in seminar series featuring academic and industry scientists.  Campus-like lifestyle with a healthy work-life balance.  Mentored opportunities to further develop professional skills.Who we areA member of the Roche Group, Genentech has been at the forefront of the biotechnology industry for more than 40 years, using human genetic information to develop novel medicines for serious and life-threatening diseases. We are a research-driven biotechnology company, whose medical innovations for cancer and other serious illnesses make a difference for patients across the globe. Please take this opportunity to learn about Genentech where we believe that our employees are our most important asset &amp;amp; are dedicated to remaining a great place to work.Genentech is an equal opportunity employer &amp;amp; prohibits unlawful discrimination based on race, color, religion, gender, sexual orientation, gender identity/expression, national origin/ancestry, age, disability, marital &amp;amp; veteran status. For more information about equal employment opportunity, visit our Genentech Careers page.Genentech is an equal opportunity employer, and we embrace the increasingly diverse world around us. Genentech prohibits unlawful discrimination based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin or ancestry, age, disability, marital status and veteran status.Genentech requires all new hires to be fully vaccinated against COVID-19 as of their start date. This requirement is a condition of employment at Genentech, and it applies regardless of whether the position is located at a Genentech campus or is fully remote.  If you are unable to receive the vaccine due to a disability or serious medical condition, or because it is prohibited as a result of your sincerely held religious beliefs, you will have an opportunity to request a reasonable accommodation.",
      "category"    : "job"
    },
  
    {
      "title"       : "Scientist Position in Computational Biology/ Bioinformatics/ AI in Epigenetics (f/m/x)",
      "url"         : "/jobs/helmholtz_scientist/",
      "description" : "Discover personalized medical solutions for environmentally triggered diseases to promote a healthier society in a rapidly changing world.The Schneider lab at the Institute of Functional Epigenetics () focuses on understanding the epigenetic mechanisms whereby cells and organisms integrate metabolic signals, establish cellular memory and regulate plasticity. We use cutting-edge methods (multi-omics, single cell, modelling, chromatin biochemistry) and model systems from yeast to stem cells to predict, observe, and manipulate epigenetic processes.We are offering aScientist Position in Computational Biology/ Bioinformatics/ AI in Epigenetics (f/m/x)101610Full timeNeuherberg near MunichPostdocsYour responsibilitiesThe position will involve the analysis and integration of multiple and large datasets to understand genome function, epigenetic memory and transcriptional regulation. The successful candidate will be tasked with the development, implementation and standardisation of low-input or single cell genomic pipelines, the integration of data from several “omics “datasets and the introduction of machine learning and AI approaches to understand epigenetic regulation of chromatin function. The applicant will be part of a computational team with close collaborations with wet-lab scientists and leading Computational Biology/AI in Health groups at Helmholtz Munich. Working language is English.Your QualificationsYou should have a Ph.D in Bioinformatics, Biostatistics, Computational Biology, Systems Biology or Molecular Biology with a previous extensive experience in bioinformatics, computational analyses and data integration. We specifically look for independent thinkers, who are willing to shape a project by developing their own ideas. Experience with NGS datasets (e.g. ChIPseq, CUT&amp;amp;Run, or CUT&amp;amp;Tag, ATACseq and RNAseq) as well as their relevant data analysis tools and pipelines is mandatory. Proficiency in scripting languages, experience with workflow management systems and high-performance computing cluster environments would be an advantage. Knowledge of molecular gene regulation, chromatin and epigenetics are beneficial.What we offer youwork-life balanceflexible working hours &amp;amp; working-time modelscontinuous education and training30 days annual leaveon-site health management servicehome office optionson-site nursery &amp;amp; holiday careelder care company pension scheme discounted public transport ticketMunich, with its numerous lakes and its vicinity to the Alps, is considered to be one of the cities with the best quality of life worldwide. With its first-class universities and world-leading research institutions it offers an intellectually stimulating environment.Provided that the prerequisites are fulfilled, a salary level up to E 13 is possible. Social benefits are based on the collective agreement for the federal public service (TVöD). The position is (initially) limited to two years, under certain circumstances an extension can be arranged.To promote diversity, we welcome applications from talented people regardless of gender, cultural background, nationality, ethnicity, sexual identity, physical abilities, religion and age. Qualified applicants with physical disabilities will be given preference.If you have obtained a university degree abroad, we require further documents from you regarding the recognition of the degree.Interested? If you have further questions, simply contact Prof. Dr. Robert Schneider, 089 3187-3586, who will be happy to be of assistance. Helmholtz Zentrum MünchenDeutsches Forschungszentrum für Gesundheit und Umwelt (GmbH)Institute of Functional EpigeneticsIngolstädter Landstraße 185764 OberschleißheimTotal E-Quality Award for excellent gender equality policy for women and men: Helmholtz Munich is particularly committed to promoting professional equality between women and men. It therefore aims to increase the proportion of the underrepresented sex in the respective field. Helmholtz Munich is part of the Helmholtz Association, Germany’s largest scientific organization. Altogether 43.000 people currently work in its 18 scientific-technical and biological-medical research centers. The Association’s annual budget amounts to around 5 billion Euros.",
      "category"    : "job"
    },
  
    {
      "title"       : "Open Full Professorship in High Performance Computing",
      "url"         : "/jobs/huberlin_professorship_hpc/",
      "description" : "The Department of Computer Science at Humboldt-Universität zu Berlin is seeking an outstanding researcher to be appointed as a full-time, tenured Full-Professor (W3) on “High Performance Computing”. It is planned that the appointed professor also serves as vice-president at Zuse Institute Berlin (ZIB), heading the research unit “Parallel and Distributed Computing“ and the National High Performance Computing Center at ZIB.We are looking for a researcher with an excellent track record in modern high performance computing (HPC). We are particularly interested in applicants with demonstrated experience and expertise in the development of innovative methods and algorithms on the software- and hardware-level, especially at the intersection of simulation, optimization, and machine learning. Preference will further be given to applicants with comprehensive contributions to support HPC applications, experience in the operation and evolution of an HPC center and large-scale research projects, as well as strong leadership skills.At the ZIB, the incumbent is expected to serve as vice-president. This includes the responsibility for the research unit on parallel and distributed computing and for the National High Performance Computing Center, comprising more than 50 staff members.Further information about the position can be found athttps://hu.berlin/w3_hpcFor inquires, please also reach out to Ulf Leser at leser@informatik.hu-berlin.de",
      "category"    : "job"
    },
  
    {
      "title"       : "PhD Positions in Cloud-Edge Computing",
      "url"         : "/jobs/innsbruck_phd/",
      "description" : "About the jobThe Institute of Computer Science at the University of Innsbruck, Austria seeks for excellent PhD students to carry out research in Cloud, Edge and IoT computing.Your profileYou are enthusiastic about developing and investigating innovative research ideas and systems within the discipline of distributed systems? You have a master’s or equivalent degree in computer science with excellent performance? You are a highly skilled Java programmer? You have good knowledge in programming with scripting languages? You have a strong background in distributed systems, Cloud computing and virtualization techniques? You stand out from your peers because of strong commitment and independent work? You are a team player and communicative (excellent oral and written English skills, good ability to write scientific publications)?Required Skills  Java (expert programmer)  Docker  git and GitHub  experience with cloud providers, e.g. AWSPreferred Qualifications  Eclipse VertX  google guice  JUnit  Gradle  scripting language (e.g. python or node/typescript)  TerraForm  Serverless (FaaS) platform  edge and IoT computing infrastructures  Experience with ML/RL/DL  event-driven systemsOur offerSuccessful candidates will join a dynamic, international and world-wide highly regarded research team. As fully funded PhD students they will investigate novel Cloud, Edge and Fog techniques that are built based on Java and modern virtualization methods. The particular research topics of interest include  robust, secure and service agnostic resource management,  data and work distribution,  serverless architectures for distributed applications  AI-enabled cloud-edge framework and cognitive services  dynamic allocation of cloud services,  automatic discovery and composition of services,  orchestration and usage of services,  scheduling and optimization,  resilient and secure runtime environments,  application development environments, and  programmingfor Cloud, Fog and Edge infrastructures.The research of these PhD positions will be conducted as part of the APOLLO (application orchestration and runtime framework for leveraging the edge-cloud continuum). Our PhD students are given the opportunity to work with some of the most advanced Cloud, Fog and Edge infrastructures in Europe and gain interdisciplinary expertise by participating in national and international (EU funded H2020) projects. Note that the working and study language as well as the entire PhD course and research program are held in English only. There is no need to learn German for these positions.The University of Innsbruck (UIBK)Founded in 1669 with several Nobel Prize winners, today the University of Innsbruck is the largest educational institution in Western Austria ranked as an international top-200 University in Computer Science. UIBK has a long history in distributed systems, and has been involved in a substantial number of national and international distributed systems projects. We are developing the Apollo application development and computing environment (https://apollowf.github.io). We have coordinated several EU projects on distributed and parallel systems including the edutain@grid, AllScale and the ENTICE project. We are currently investigating the cloud/fog/edge continuum which is changing from a pure elastic provisioning of virtual resources to a transparent and adaptive hosting environment that fully realizes the “everything as a service” provisioning concept, from centralized cloud to the edge and from network and computing infrastructure up to the application layer.Innsbruck and its EnvironmentThe City of Innsbruck, which hosted the Olympic winter games twice, is located in the beautiful surroundings of the Tyrolean Alps. The combination of the Alpine environment and the urban life in this historically grown town provides a high quality of living.Your applicationCandidates should submit their application as soon as possible.Application documents  Motivation letter (Why does your expertise and vision fit the profile of the open PhD position?)  Full CV including at least 2 references  Copy of BSc and MSc degrees  Transcripts for all study programs  If available provide TOEFL and GRE test resultsAll documents must be submitted in English. The documents must be merged into a single zip file and sent to thomas.fahringer@uibk.ac.at (subject line: Full-time PhD Position in Cloud computing). For additional files such as theses and publications please add a link to a cloud repository and make it accessible. The candidate will receive an e-mail confirming receipt of the application.Application Process and Interview  Interviews will take place in stages as soon as possible, perhaps via Skype.  Applicants are encouraged to apply immediately as the position will be filled upon finding the right candidate.  We reserve the right to hold applications on file for potential future job openings.Please direct questions to:Prof. Dr. Thomas FahringerInstitute of Computer Science, University of InnsbruckTechnikerstr. 21a, A-6020 Innsbruck, AustriaEmail: Thomas.Fahringer@uibk.ac.atURL: https://dps.uibk.ac.at",
      "category"    : "job"
    },
  
    {
      "title"       : "Bioinformatics Pipeline Developer",
      "url"         : "/jobs/naturemetrics_bioinformatics_pipeline_developer/",
      "description" : "Applications are invited for a Bioinformatics Pipeline Developer to join an expanding science-based SME. The successful candidate will work closely with other members of the Bioinformatics department on developing cloud-native, secure, scalable infrastructure for our rapidly growing business. The role will report to the Senior Bioinformatics Engineer and be a critical component of the Bioinformatics Infrastructure team.Applicants should have experience of developing, optimising, orchestrating, and scaling bioinformatics pipelines in the cloud, or on significant HPC infrastructure. A good understanding of containerisation strategies, CI/CD and testing frameworks is desirable.The successful applicant will be comfortable working independently and as part of a wider team. They will be required to work closely with all members of the Bioinformatics team, taking requirements and input from our Bioinformatics Product Delivery and Bioinformatics R&amp;amp;D teams to ensure we are able to meet our scale up challenges.NatureMetrics is a high growth SME leading the revolution in molecular biodiversity monitoring, enabling environmental managers to measure and monitor biodiversity with DNA-based tools. NatureMetrics has grown rapidly over the last five years and validated our technology in multiple industry sectors and regions of the world. We now have an exciting scale up plan backed by ambitious and supportive investors. We are a team of bright, enthusiastic individuals who are excited to be breaking new ground and disrupting the world of biodiversity monitoring. We take great pride in our work and are seeking new team members who will do the same.The full specification can be found at NatureMetrics Breezy - Bioinformatics Pipeline Developer. There is no fixed deadline for applications, and the position will be held open until we find the right candidate.",
      "category"    : "job"
    },
  
    {
      "title"       : "Computer Scientist",
      "url"         : "/jobs/ornl_computer_scientist/",
      "description" : "OverviewOak Ridge National Laboratory (ORNL) is the United States Department ofEnergy’s largest science and energy laboratory tasked to provide the UnitedStates government agencies and departments with technology and expertise tosupport national and homeland security needs. Our diverse capabilities spanscientific and engineering disciplines, enabling the Laboratory to explorefundamental science challenges and to carry out the research needed toaccelerate the delivery of solutions to the marketplace. We invite applicationsfor the position of Software Engineer within the Workflow Systems Group in theData in the Artificial Intelligence Section in the Computer Science andMathematics Division. Our group works closely with application scientists,applied mathematicians, and computer scientists.  Our collaborative workconcerns all stages of the scientific life cycle and utilizes computingplatforms ranging from the desktop to next generation supercomputers. Our groupworks closely with application scientist (e.g. fusion, accelerator, seismology,nuclear), applied mathematicians, and computer scientists. Our group focuses onresearch and software artifacts in highly cited journals and conferences,helping to enable bleeding edge science at scale. Our emphasis is on team work,and it is important for the applicant to show a strong desire to work within ateam, as well as promote her/his research.Job Duties and Responsibilities:  Make direct contributions working closely with project teams to create, evaluate, and publish novel research ideas.  Mentor students and postdocs.  Publish papers in high-quality refereed conferences and journals.  Collaborate with industry, academia, government labs.  Participate in proposal development for potential external and internal funding.Basic Qualifications:  Ph.D. in computer science, computational science or a discipline related to the job duties.  Experience in parallel computing  Experience with storage systems  Strong programming skills in C/C++ and/or Python  Previous research experience in ONE or more of the following areas is also required:          I/O      Workflows      Data Science      Using machine Learning to solve complex problems      Data Compression      Performance Modeling and/or Tuning of HPC codes      Computer Systems      Preferred Qualifications:  Excellent interpersonal skills, oral and written communication skills, and strong personal motivation  The ability to work in a dynamic, interdisciplinary team  The ability to take initiative on research insights to bring them to fruition through publication or demonstration on mission applications",
      "category"    : "job"
    },
  
    {
      "title"       : "Scientific Software Engineer",
      "url"         : "/jobs/ornl_scientific_software_engineer/",
      "description" : "OverviewOak Ridge National Laboratory (ORNL) is the United States Department ofEnergy’s largest science and energy laboratory tasked to provide the UnitedStates government agencies and departments with technology and expertise tosupport national and homeland security needs. Our diverse capabilities spanscientific and engineering disciplines, enabling the Laboratory to explorefundamental science challenges and to carry out the research needed toaccelerate the delivery of solutions to the marketplace. We invite applicationsfor the position of Software Engineer within the Workflow Systems Group in theData in the Artificial Intelligence Section in the Computer Science andMathematics Division.Job Duties and Responsibilities:  Contribute to the development of one or more of ORNL software products developed and maintained by the Workflow Systems Group (ADIOS, MGARD, EFFIS, …).  Utilize well established software engineering principles to create and harden ORNLs software technologies.  Work closely with customers to meet their software requirements, address bugs, and achieve their scientific goals.Basic Qualifications:  A minimum of a BS Degree in computer science, or a related field.  2+ years of experience in modern object-oriented programming languages including C++, Python, outside of degree.  Experience with parallel I/O.  Demonstrated software development experience in computational science, engineering, computer science, or applied math.  The ability to work on a team in a dynamic group environment as the team meets daily, practices Continuous Integration, collaborates on the source code and has regular deadlines.  Experience in parallel and/or distributed computing.  Strong programming skills in C/C++Preferred Qualifications:  MS and 2+ years of software engineering experience outside of the degree.  Experience working with scientific application teams.  Experience with software development methodologies, such as version control systems like Git and Subversion and knowledge of UML.  Excellent communication and writing skills  Prior experience developing or contributing to large, complex software systems.  Experience in large scale data management.  Activity within the broader open-source software community would be looked upon favorably, but is not required.  Regular presentations or tutorials at relevant conferences.  A track-record in developing detailed documentation in multiple formats",
      "category"    : "job"
    },
  
    {
      "title"       : "Senior Scientific Software Engineer",
      "url"         : "/jobs/ornl_senior_scientific_software_engineer/",
      "description" : "OverviewOak Ridge National Laboratory (ORNL) is the United States Department of Energy’s largest science and energy laboratory tasked to provide the United States government agencies and departments with technology and expertise to support national and homeland security needs. Our diverse capabilities span scientific and engineering disciplines, enabling the Laboratory to explore fundamental science challenges and to carry out the research needed to accelerate the delivery of solutions to the marketplace. We invite applications for the position of Senior Software Engineer within the Workflow Systems Group in the Data in the Artificial Intelligence Section in the Computer Science and Mathematics Division.Job Duties and Responsibilities  Contribute to the development of one or more of ORNL software products developed and maintained by the Workflow Systems Group (ADIOS, MGARD, EFFIS, …).  Utilize well established software engineering principles to create and harden ORNLs software technologies.  Work closely with customers to meet their software requirements, address bugs, and achieve their scientific goals.Basic Requirements  A minimum of a BS Degree in computer science, or a related field.  8+ years of experience in modern object-oriented programming languages including C++, Python, outside of degree.  Experience with parallel I/O.  Demonstrated software development experience in computational science, engineering, computer science, or applied math.  The ability to work on a team in a dynamic group environment as the team meets daily, practices Continuous Integration, collaborates on the source code and has regular deadlines.  Experience in parallel and/or distributed computing.  Strong programming skills in C/C++Preferred Requirements  MS and 7+ years of software engineering experience outside of the degree.  Experience working with scientific application teams.  Experience with software development methodologies, such as version control systems like Git and Subversion and knowledge of UML.  Excellent communication and writing skills  Prior experience developing or contributing to large, complex software systems.  Experience in large scale data management.  Activity within the broader open-source software community would be looked upon favorably, but is not required.  Regular presentations or tutorials at relevant conferences.  A track-record in developing detailed documentation in multiple formats",
      "category"    : "job"
    },
  
    {
      "title"       : "Assistant Professor of Computer Science in the area of Cloud, Edge and IoT Computing",
      "url"         : "/jobs/university_innsbruck_assistant_professor/",
      "description" : "About the jobThe Department of Computer Science (at the Faculty of Mathematics, Computer Science and Physics) at the University of Innsbruck seeks to fill the position of a full-time Assistant Professor of Computer Science in the area of Cloud, Edge and IoT Computing as soon as possible, ideally by Jan. 1, 2023. This position is initially limited to 6 years; a tenure-track agreement can be offered within the first year of employment. Upon positive evaluation, the position is converted into a tenured Associate Professorship. This career position is embedded within an attractive environment of existing competencies close to the above thematic area, including distributed systems, parallel systems, software engineering, data management, security and privacy.TasksThe successful applicant should represent the area of Cloud, Edge and IoT Computing in research and teaching. Here, Cloud, Edge and IoT Computing refers to Distributed Systems with a particular focus on  application development environments and tools  scalable decentralized runtime systems and middleware  optimization of applications and runtime systems  decentralized management and allocation of compute and data resources  distributed processing and management of large-scale data  distributed scheduling, resilience, and self-organization  service composition, provisioning, and orchestration  mobile computing, and autonomous agentsTeaching comprises lectures, especially in the area of distributed systems, for the degree programs of the Department of Computer Science. Tenured Associate Professors are also expected to teach mandatory courses in the Bachelor’s program.Acquisition and scientific management of third-party funded research projectsSupervision and co-supervision of Bachelor’s, Master’s and PhD studentsParticipation in organizational and administrative tasks as well as in evaluation measuresRequired Qualifications  Doctoral degree in computer science or a related field;  Postdoctoral work experience after the dissertation/PhD;  Pertinent and independent scientific achievements beyond the dissertation/PhD;  Research experience and international visibility in the area of Cloud, Edge and IoT Computing;  Relevant publications in leading international, refereed conference proceedings and journals as well as presentations at international conferences and workshops;  Research collaborations with international partners;  Experience in the acquisition and implementation of externally-funded research projects;  Experience in the (co-)supervision of Bachelor’s and Master’s theses;  Pertinent experience in academic mobility during or after the PhD;  Proficiency (written and oral) in English;  Willingness to teach in German within two years after appointment;  Didactic competence and demonstrated teaching experience;  Social competencies, communication skills, and ability to work in teamsHow to ApplyThe application must be submitted in English and must contain:  CV with a description of the academic and professional career;  List of scientific publications;  List of scientific presentations, other scientific achievements, and projects;  Names and contact information of at least two references;  Description of a research plan at the level of a habilitation;  Teaching statement and a list of courses taught with evaluations (if available)The University of Innsbruck strives to increase the proportion of its female employees, especially in leadership positions, and therefore explicitly invites women to apply. In the case of equivalent qualifications, female applicants will be given preference.The annual gross salary is € 56,868 at the time of employment. It is raised to € 64,394 if the tenure-track agreement is signed within the first year of employment, and once again if the qualification goals are completed (€ 69,820 as of 2022).The documents must be merged into a single PDF file and sent to Thomas.Fahringer@uibk.ac.at (subject line: Assistant Professor in Cloud, Edge and IoT Computing) by Sept 30, 2022. For additional files please add a link in the submitted pdf to a cloud repository and make it accessible. The candidate will receive an e-mail confirming receipt of the application.Direct questions toProf. Dr. Thomas FahringerInstitute of Computer Science, University of InnsbruckTechnikerstr. 21a, A-6020 Innsbruck, AustriaEmail: Thomas.Fahringer@uibk.ac.atURL: https://dps.uibk.ac.at",
      "category"    : "job"
    },
  
    {
      "title"       : "Assistant Professor of Computer Science in the area of Cloud, Edge and IoT Computing",
      "url"         : "/jobs/university_innsbruck_assistant_professor_2022/",
      "description" : "About the jobThe Department of Computer Science (at the Faculty of Mathematics, Computer Science and Physics) at the University of Innsbruck seeks to fill the joint position of a full-time Assistant Professor of Computer Science in the area of Cloud, Edge and IoT Computing as soon as possible, ideally by 01.12.2022. This position is initially limited to 6 years; a tenure-track agreement can be offered within the first year of employment. Upon positive evaluation, the position is converted into a tenured Associate Professorship. This career position is embedded within an attractive environment of existing competencies close to the above thematic area, including distributed systems, parallel systems, software engineering, data management, security and privacy.TasksThe successful applicant should represent the area of Cloud, Edge and IoT Computing in research and teaching. Here, Cloud, Edge and IoT Computing refers to distributed systems with a particular focus on highly decentralized, scalable, performance-oriented, and resilient application development environments, runtime systems and resource management. Example research foci include:  application development environments and tools  scalable, decentralized runtime systems and middleware  optimization of applications and runtime systems (e.g. for runtime, energy, cost and resilience)  decentralized and energy-aware management and allocation of compute and data resources (e.g. based on OpenStack)  distributed processing, management, and transfer of large-scale data  distributed scheduling, resilience, and self-organization  instrumentation, monitoring and analysis of the non-functional behavior (e.g. time, energy, cost, reliability) of application, runtime, and resource layer  service composition, provisioning, and orchestration (e.g. based on Kubernetes or FaaS technologies)  mobile computing and autonomous agentsTeaching comprises lectures, especially in the area of distributed systems, for the degree programs of the Department of Computer Science. Tenured Associate Professors are also expected to teach mandatory courses in the Bachelor’s program.Acquisition and scientific management of third-party funded research projects is expected. Supervision and co-supervision of Bachelor’s, Master’s and PhD students, as well as the participation in organizational/administrative tasks and in evaluation measures is a matter of course.Required Qualifications  Doctoral degree in computer science or a related field;  Postdoctoral work experience after the dissertation/PhD;  Pertinent and independent scientific achievements beyond the dissertation/PhD;  Experience in research and SW development, and international visibility in the area of Cloud, Edge and IoT computing;  Relevant publications in leading international, refereed conference proceedings and journals as well as presentations at international conferences and workshops;  Research collaborations with international partners;  Experience in the acquisition and implementation of externally-funded research projects;  Experience in the (co-)supervision of Bachelor’s and Master’s theses;  Pertinent experience in academic mobility during or after the PhD;  Proficiency (written and oral) in English;  Willingness to teach in German within two years after appointment;  Didactic competence and teaching experience expected;  Social competencies, communication skills, and ability to work in teamsHow to ApplyThe application must be submitted in English and must contain:  CV with a description of the academic and professional career;  List of scientific publications;  List of scientific presentations, other scientific achievements, and projects;  Names and contact information of at least two references;  Description of a research plan at the level of a habilitation;  Teaching statement and a list of courses taught with evaluations (if available)The University of Innsbruck strives to increase the proportion of its female employees, especially in leadership positions, and therefore explicitly invites women to apply. In the case of equivalent qualifications, female applicants will be given preference.The annual gross salary is € 56,868* at the time of employment. It is raised to € 66,952* if the tenure-track agreement is signed within the first year of employment, and once again if the qualification goals are completed (€ 72,457). Tenured Associate Professors further advance on a defined pay scale up to a maximum of € 101.252 (*as of 2022)We look forward to receiving your online application (Chiffre MIP 12846) until 09.09.2022 at https://lfuonline.uibk.ac.at/public/karriereportal.details?asg_id_in=12846The legally binding text in German is available at: https://lfuonline.uibk.ac.at/public/karriereportal.details?asg_id_in=12846",
      "category"    : "job"
    },
  
    {
      "title"       : "Research Engineer I",
      "url"         : "/jobs/usc_research_engineer/",
      "description" : "About UsThe SciTech group does research and development on software systems to help scientists manage large-scale computations. We work with scientists in domains ranging from genomics and proteomics to seismology and gravitational wave physics. We help scientists deploy their computations on some of the largest and fastest computing systems in the world. Our main project is the Pegasus Workflow Management system, which is used to orchestrate complex, large-scale data processing and computation pipelines. Pegasus compiles abstract, high-level workflow descriptions into efficient executable workflows that can be deployed on diverse cyberinfrastructure. In this process it expands and optimizes the workflow, adding data management tasks and performing task clustering, cleanup and other optimizations.For more information about our group please visit our website: http://pegasus.isi.eduThe SciTech group is part of the USC Information Sciences Institute (ISI). A unit of the University of Southern California’s highly ranked Viterbi School of Engineering, ISI is one of the nation’s largest, most successful university-affiliated computer research institutes. ISI researchers bridge the gap between theoretical basic research and product-oriented research and development. Its diverse expertise ranges from core engineering and computer science discovery to design, modeling and implementation of innovative prototypes and devices. ISI is located in beautiful, sunny Marina Del Rey, California. Our offices overlook the ocean and are just minutes from the beach.Job DescriptionResponsibilities  Develops clean, reliable, testable, documented code  Writes clear documentation for users  Works with domain scientists to develop computational workflows  Communicates effectively with users and developers to resolve technical problems  Support academic research efforts (present work at workshops, contribute to academic papers)  Provides support and facilitation servicesRequired Skills  Significant experience with one of the following object oriented programming languages:, Python, Java, C++, Go  Strong Linux skills, command line interfaces, shell scripting, and system administration  Experience with a version control system such as Git  Bachelor’s degree in Computer Science or related field requiredPreferred Skills  Experience working with scientific computing applications and scientific workflows  Knowledge of high-performance, high-throughput, or data-intensive computing  Understanding of devops, container orchestration (Kubernetes, Docker, …) and cloud technologies (AWS, GCE, Azure,…)  Experience supporting users via ticket or bug tracking systemsThe University of Southern California values diversity and is committed to equal opportunity in employment.Minimum Education: Bachelor’s degree Minimum Experience: 2 years Minimum Field of Expertise: Sound knowledge of programming and documentation procedures, and programming methods. Knowledge of one or more appropriate computer languages.",
      "category"    : "job"
    },
  
  
    {
      "title"       : "Workflows Education:  Underlying  Computer Science Concepts",
      "url"         : "/stories/2022/05/16/workflow-education/",
      "description" : "Workflow applications have become mainstream in many domains, including most of the sciences, engineering, as well as AI. It is thus crucial that educational and training opportunities be available to help grow an effective workflow workforce. Several institutions have already developed and made available training material for particular workflow systems, so that users can learn how to deploy and execute their workflow applications on hardware platforms on which these systems are installed. Less developed, but no less crucial, are pedagogic materials that target the fundamental concepts necessary to understand workflow applications and reason about their executions and the performance thereof. One of the reasons why these pedagogic materials are less developed is that many of the relevant concepts belong to “standard” parallel and distributed computing (PDC) topics, and it is assumed that these topics are already covered in university computer science curricula.This assumption is problematic for several reasons. First, it is well recognized that PDC is not sufficiently included in undergraduate computer science curricula, which has motivated the establishment of the NSF/IEEE-TCPP Curriculum Initiative on Parallel and Distributed Computing. Although progress is being made, many computer science college graduates still do not have sufficient, or any, PDC exposure. Second, many potential members of the workflow workforce will not be computer science graduates. Third, even if students have been taught the relevant concepts at different times throughout their education, they may not have retained them effectively. There is a thus need for a one-stop, self-contained ``these are the concepts needed for being able understand and reason about workflow executions” pedagogic package. This content of this package should draw from multiple sources, and should be curated and vetted by the Workflow Community Initiative. Its main use would be to provide “prerequisite concepts you need to know for workflows” to students (as a complement to or a component of university courses, before starting a workflow-related internship, before engaging on graduate-level workflow-related projects, etc.) and professionals (e.g., before beginning to use workflows, to better understand workflow behavior and performance in various professional contexts).NSF/IEEE-TCPP Curriculum Initiative on Parallel and Distributed Computing.A project that could provide useful components to include in this envisioned pedagogic package is EduWRENCH. EduWRENCH provides many pedagogic modules, each one including a pedagogic narrative, practice questions, open questions, and in-the-browser simulation-driven activities. Basic EduWRENCH modules target fundamental concepts of computation, I/O, and networking and explain how they drive application performance. Some modules focus on principles of parallel computation on multi-core machines, including notions of parallelism, speedup, efficiency, overhead, and load balancing. Other modules focus on principles of distributed computation over a network, including notions of data proximity. Several more advanced build on the aforementioned modules to teach workflow concepts and/or to use workflows as case-studies for more advanced topics such as scheduling, energy efficiency, etc. Most EduWRENCH modules have already been used effectively not only in university courses, but also to train beginning graduate students who are about to join a workflow research and development group. EduWRENCH by no means provides a comprehensive pedagogic package for workflows, but it may serve as a good starting point for the Workflow Community Initiative to define what such a package should and should not contain.EduWRENCH website.",
      "category"    : "story"
    },
  
    {
      "title"       : "A quick overview of Nextflow workflow system",
      "url"         : "/stories/2022/09/28/nextflow/",
      "description" : "Workflow management systems are as diverse as the business and scientific processes they support – from engineering to research to process automation. This article describes Nextflow – an open-source workflow manager widely used in life sciences. It covers the motivations behind Nextflow, explains what it is, and describes what the future holds for the platform.Background and MotivationsLike similar open source efforts, Nextflow was born out of a need to solve specific problems while I was working as a research engineer in the Comparative Bioinformatics labs at the Centre for Genomics Regulation (CRG). Researchers were struggling with several issues that are all likely too familiar to workflows community members – complex, buggy scripts, long-running workflows that would suddenly fail, and challenges monitoring, managing, and maintaining workflows.While there were several available workflow managers at the time, none of them specifically addressed our requirement. A fundamental challenge in the lab at that time was data handling. Comparative bioinformatics involves studying genome and protein sequences across species, and population-level studies can involve massive amounts of data. We needed a simple yet powerful framework to deploy the executions of thousands of tasks, comparing different alignment methods and protein sequences each other.Some Key Ideas Behind NextflowNextflow was designed from scratch having clear in mind key ideas and best practices:  Allow developers to reuse any existing piece of software without the need of intermediate interface or wrapper; the tool command line is the interface and Linux is the integration layer.  Manage tasks as a functional and self-contained unit of work. This was a key requirement to enable the deployment across heterogeneous computing platforms and allow auto-retry execution policy on failure.  Provides a high-level abstraction for tasks parallelisation that allows developers to write simple yet high-scalable application, without the need to struggle with low level problem such as race conditions  and locks to access shared resources.  Strongly decouple the scientific application logic from the configuration &amp;amp; deployment setting, in order to streamline the deployment across different platforms and enable the migration to cloud environments.  Remove unnecessary dependencies with external services and databases. We wanted a zero configuration experience both for the Nextflow runtime and the resulting pipelines.  Enable debugging and recovering of failed executions. Bioinformatics pipeline can spin the execution of tens of thousands tasks. If something breaks we need a strategy that would allow the debugging of the failed task independently of the rest of the pipeline, and make it possible to recover the computation once the problem was solved from the last successfully computed tasks, to avoid throwing away days of computing resources.Between these, very likely, the most important design choice that distinguishes Nextflow compared to other workflow management systems is the adoption of the data flow programming paradigm.We often imagine workflows as a sequence of steps designed from the top down and including various dependencies, decision points, and sub-flows. There is another way to envision workflows, however, and that is from the perspective of the individual process steps. Individual steps have no notion of the overall flow. They have an input, perform some processing, and write data to an output, typically another step in the workflow.Using this programming model you can think a workflow behaving like spreadsheet. In a spreadsheet, users enter expressions in cells that depend on calculations in other cells. The dependencies between cells can be complex, but users don’t think about how the spreadsheet will sequence calculations. Instead, they concentrate on the logic in each cell. When a cell changes, the spreadsheet worries about how to propagate changes to dependent cells. Nextflow is described as reactive because task execution is triggered when inputs change or become valid. It turns out that this model works surprisingly well at scale, and opportunities for parallelism occur naturally without the workflow designer needing to think about them.From a technical point of view, Nextflow processes (aka tasks) can be thought of as reactive agents which run  in parallel waiting for the input data that trigger their execution. It’s important to highlight that each of these processes are isolated from each other, and they can only communicate via asynchronous messages represented by dataflow variables.Along with these, another pillar component of Nextflow was the adoption of containers as a core feature of the framework. Nextflow abstracts away the containerisation of the pipeline execution in a declarative manner. This means the user is only required to specify the container that needs to be used to run specific workflow tasks. Nextflow takes care to use this information to run the task within the container depending on the target execution platform that can be, for example, AWS Batch, an HPC batch scheduler e.g. Slurm or a local computer. This choice was proven to be critical to enable the portability and reproducibility of the resulting data analysis workflow. Nextflow nowadays supports multiple container runtime technologies, including Docker, Podman, Singularity, Charliecloud among others.Nextflow TodayWhen Nextflow was launched as an open-source project in 2013, we couldn’t have imagined what it has become today. Today, Nextflow is downloaded over 55,000 times monthly and used by over 1,000 organizations, including some of the world’s largest pharmaceutical firms. Ideas like containers and support for source code managers (SCMs) are so thoroughly baked into Nextflow’s design that they seem commonplace.In Nextflow, we were careful to decouple workflow logic from the details of underlying computing environments. To achieve this, Nextflow supports an abstraction called an Executor. Executors are pluggable components that enable pipelines to run without modification across virtually any compute environment, from a local host to an on-prem HPC cluster to various cloud services. Shifting to a new compute environment is as simple as changing a few lines of code in a configuration file.Pipelines can be stored locally or pulled from a preferred SCM at runtime. While applications can still be installed locally, pulling containers encapsulating bioinformatics tools has become the norm. Nextflow handles all the details, including compute resources, data movement, and making datasets visible to containers at runtime. Intermediate results and data are also cached, making flows resilient and recoverable after a failed step.The nf-core CommunityThe nf-core community launched in 2018 marked a key milestone for the Nextflow community. nf-core is an independent effort to collect a curated set of analysis pipelines built using Nextflow. Significant effort has gone into developing tools, templates, and guidelines that enable domain experts to contribute to the community. The result is a set of high-quality pipelines that are portable, reproducible, fully documented, and cloud-ready. A recent State of the Workflow 2022 community survey showed that 62% of Nextflow users take advantage of these pipelines in their day-to-day research – a testament to their utility and the importance of this effort.Towards the FutureWe are fortunate to have vibrant user and developer communities actively engaged in Nextflow and helping to guide its evolution. We continue to support new computing environments, introduce new functionality, and improve throughput and scalability. I recently shared some plans for Nextflow in the article Evolution of the Nextflow runtime.We are also planning to further enhance the support for containers providing a better automation on the overall container management lifecycle and ease the provisioning of multi-containers required by modern data analysis pipelines.We are also improving our commercial Tower offering enabling improved collaboration, new data-related features, and features aimed at helping further optimize resource usage to reduce cloud spending.You can learn more about Nextflow or download it for free by visiting nextflow.io. Existing Nextflow users can test-drive Nextflow Tower in the cloud at tower.nf.",
      "category"    : "story"
    },
  
    {
      "title"       : "Widening Workflows usage: the eFlows4HPC project",
      "url"         : "/stories/2022/09/29/eflows4hpc/",
      "description" : "The European High-Performance Computing Joint Undertaking (EuroHPC JU) aims at developing a World Class Supercomputing Ecosystem in Europe and, with this goal, is procuring and deploying pre-exascale and petascale systems in Europe. These systems will be capable of running large and complex applications. In this sense, the demand from the application stakeholders includes not only aspects related to High-Performance Computing (HPC) but also artificial intelligence (AI) and data analytics.The eFlows4HPC project has as objective to provide a software stack that makes easier the development of workflows that involve HPC, AI and data analytics components. The project aims to give support to dynamic workflows in the sense that the set of nodes in the graph of the workflow can change during its execution due to changes in the input data or context of the execution, and be reactive to events that may occur. The runtime systems supporting this execution should be able to perform efficient resource management, both in terms of time and energy.Another objective of the project is to provide mechanisms to make the use and reuse of HPC easier by wider communities. For this purpose, the HPC Workflows as a Service (HPCWaaS) methodology has been proposed. The goal is to provide methodologies and tools that enable sharing and reuse of existing workflows, and that assist when adapting workflow templates to create new workflow instances.eFlows4HPC software stack.The eFlows4HPC software stack includes components to develop the workflows at three levels: high-level topology of the workflows using extended TOSCA, required data transfers with the Data Logistic Pipelines and the computational aspects of the workflow with PyCOMPSs. Once a workflow has been developed, it is registered in the Workflow Registry. Similarly, the different components of the workflows, pre-trained AI models and data sets are registered in a set of catalogues and repositories. All these registries and catalogues are used by the HPCWaaS interface, which provides a REST API to deploy and execute the workflows. On execution, the stack also provides a set of runtime libraries to support the workflow execution and data management.HPCWaaS methodology.The eFlows4HPC developments are demonstrated in the project through three pillar applications in the areas of digital twins for manufacturing, climate modeling and prediction and urgent computing for natural hazards. Pillar I deals with the construction of digital twins for the prototyping of complex manufactured objects integrating state-of-the-art adaptive solvers with machine learning and data-mining, contributing to the Industry 4.0 vision. Pillar II develops innovative adaptive workflows for climate and for the study of tropical cyclones in the context of the CMIP6 experiment, including in-situ analytics. Pillar III explores the modelling of natural catastrophes – in particular, earthquakes and their associated tsunamis shortly after such an event is recorded.Coordinated by the Barcelona Supercomputing Center (BSC), the eFlows4HPC consortium comprises 16 partners from seven different countries with expertise in technical aspects:  supercomputing and acceleration, workflow management and orchestration, machine learning, big data analytics, data management, and storage; together with expertise in the pillar workflows’ areas: manufacturing, climate, urgent computing.The initial results of the project are available under the project website in its deliverable section (see https://eflows4hpc.eu/deliverables/). The source code and documentation are publicly available as well (see https://github.com/eflows4hpc and https://eflows4hpc.readthedocs.io/en/latest/).",
      "category"    : "story"
    },
  
  
    {
      "title"       : "IPDPS 2023 (Conference)",
      "url"         : "https://www.ipdps.org/",
      "description" : "37th IEEE International Parallel & Distributed Processing Symposium",
      "category"    : "event"
    } ,
  
    {
      "title"       : "CCGrid 2023 (Conference)",
      "url"         : "http://cds.iisc.ac.in/faculty/simmhan/ccgrid2023//",
      "description" : "23rd IEEE/ACM international Symposium on Cluster, Cloud and Internet Computing",
      "category"    : "event"
    } ,
  
    {
      "title"       : "HTCondor Week (Conference)",
      "url"         : "https://indico.cern.ch/event/1174979/",
      "description" : "European HTCondor Week 2022",
      "category"    : "event"
    } ,
  
    {
      "title"       : "Nextflow Summit (Summit)",
      "url"         : "https://summit.nextflow.io/",
      "description" : "Nextflow Summit",
      "category"    : "event"
    } ,
  
    {
      "title"       : "ERROR 2022 (Workshop)",
      "url"         : "https://error-workshop.org/",
      "description" : "2nd Workshop on E-science ReseaRch leading tO negative Results",
      "category"    : "event"
    } ,
  
    {
      "title"       : "Sci-k 2022 (Workshop)",
      "url"         : "https://sci-k.github.io/2022/",
      "description" : "2nd International Workshop on Scientific Knowledge: Representation, Discovery, and Assessment",
      "category"    : "event"
    } ,
  
    {
      "title"       : "eScience 2022 (Conference)",
      "url"         : "https://escience-conference.org/2022",
      "description" : "18th IEEE International Conference on e-Science",
      "category"    : "event"
    } ,
  
    {
      "title"       : "BOSC 2022 (Conference)",
      "url"         : "https://www.open-bio.org/events/bosc-2022/",
      "description" : "Bioinformatics Open Source Conference",
      "category"    : "event"
    } ,
  
    {
      "title"       : "HPDC 2022 (Conference)",
      "url"         : "http://www.hpdc.org/2022/",
      "description" : "31st International Symposium on High-Performance Parallel and Distributed Computing",
      "category"    : "event"
    } ,
  
    {
      "title"       : "SC'22 (Conference)",
      "url"         : "https://sc22.supercomputing.org/",
      "description" : "The International Conference for High Performance Computing, Networking, Storage, and Analysis",
      "category"    : "event"
    } ,
  
    {
      "title"       : "Euro-Par 2022 (Conference)",
      "url"         : "https://2022.euro-par.org/",
      "description" : "28th International European Conference on Parallel and Distributed Computing",
      "category"    : "event"
    } ,
  
    {
      "title"       : "CWL 2022 (Conference)",
      "url"         : "https://cwl.discourse.group/t/2022-cwl-conference-feb-28-mar-4-2022",
      "description" : "2022 Common Workflow Language Conference",
      "category"    : "event"
    } ,
  
    {
      "title"       : "GCC2022 (Conference)",
      "url"         : "https://galaxyproject.org/events/gcc2022/",
      "description" : "2022 Galaxy Community Conference",
      "category"    : "event"
    } ,
  
    {
      "title"       : "nf-core (Workshop)",
      "url"         : "https://nf-co.re/events/2022/hackathon-march-2022",
      "description" : "March 2022 Hackathon",
      "category"    : "event"
    } ,
  
    {
      "title"       : "WORKS22 (Workshop)",
      "url"         : "https://works-workshop.org/",
      "description" : "17th Workshop on Workflows in Support of Large-Scale Science",
      "category"    : "event"
    } ,
  
    {
      "title"       : "PASC 2022 (Conference)",
      "url"         : "https://pasc22.pasc-conference.org/",
      "description" : "Platform for Advanced Scientific Computing",
      "category"    : "event"
    } ,
  
    {
      "title"       : "ISC 2022 (Conference)",
      "url"         : "https://www.isc-hpc.com",
      "description" : "ISC High Performance",
      "category"    : "event"
    } ,
  
    {
      "title"       : "ReWords 2022 (Workshop)",
      "url"         : "https://sites.google.com/view/rewords22",
      "description" : "2nd Workshop on Reproducible Workflows, Data Management, and Security",
      "category"    : "event"
    } ,
  
    {
      "title"       : "FDO 2022 (Conference)",
      "url"         : "https://www.fdo2022.org/",
      "description" : "1st International Conference on FAIR Digital Objects",
      "category"    : "event"
    } ,
  
    {
      "title"       : "Parsl & funcX Fest 2022 (Workshop)",
      "url"         : "https://parsl-project.org/parslfest2022.html",
      "description" : "Parsl & funcX Fest 2022",
      "category"    : "event"
    } 
  
]
